{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdNblojx_CsU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "from pandas import DataFrame, Series\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.dates as mdates\n",
        "\n",
        "base_path = Path(__file__).parent\n",
        "CONFIG = {\n",
        "    'data_url': base_path / 'data/sp500_prices.csv',\n",
        "    'date_col': 'Date',\n",
        "    'price_col': 'Price'\n",
        "}\n",
        "\n",
        "def getDataFrame() -> DataFrame:\n",
        "    dataset = pd.read_csv(CONFIG['data_url'], parse_dates=[CONFIG['date_col']], index_col=CONFIG['date_col']).sort_index()\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def getPrice(dataframe: DataFrame) -> Series:\n",
        "    return dataframe[CONFIG['price_col']]\n",
        "\n",
        "\n",
        "def calculateMAE(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Calculate the Mean Absolute Error between true and predicted values.\n",
        "\n",
        "    Args:\n",
        "        y_true: Array of actual/true values\n",
        "        y_pred: Array of predicted values\n",
        "\n",
        "    Returns:\n",
        "        float: Mean Absolute Error value\n",
        "\n",
        "    Raises:\n",
        "        ValueError: If arrays are not 1-dimensional or have different lengths\n",
        "    \"\"\"\n",
        "\n",
        "    if y_true.ndim != 1 or y_pred.ndim != 1:\n",
        "        raise ValueError(\"Both arrays must be 1-dimensional.\")\n",
        "\n",
        "    if len(y_true) != len(y_pred):\n",
        "        raise ValueError(f\"Arrays must have the same length (y_true: {len(y_true)}, y_pred: {len(y_pred)})\")\n",
        "\n",
        "    return np.mean(np.abs(y_true - y_pred))\n",
        "\n",
        "\n",
        "def split_sequence(sequence, window_size, horizon):\n",
        "    \"\"\"\n",
        "    Split a time series sequence into input-output pairs for training.\n",
        "\n",
        "    Args:\n",
        "        sequence: List or array of sequential data points\n",
        "        window_size: Number of timesteps to use as input features\n",
        "        horizon: Number of timesteps to predict as output\n",
        "\n",
        "    Returns:\n",
        "        input_sequences: Array of input sequences, each of length window_size\n",
        "        output_sequences: Array of corresponding output sequences, each of length horizon\n",
        "    \"\"\"\n",
        "\n",
        "    X, y = [], []\n",
        "\n",
        "    for start_index in range(len(sequence)):\n",
        "        end_index = start_index + window_size\n",
        "        output_end_index = end_index + horizon\n",
        "\n",
        "        if output_end_index > len(sequence):\n",
        "            break\n",
        "\n",
        "        input_sequence = sequence[start_index:end_index]\n",
        "        output_sequence = sequence[end_index:output_end_index]\n",
        "\n",
        "        X.append(input_sequence)\n",
        "        y.append(output_sequence)\n",
        "\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "\n",
        "def split_train_test(raw_sequence, horizon):\n",
        "    \"\"\"\n",
        "    Split a time series sequence into training and test sets.\n",
        "\n",
        "    Args:\n",
        "        raw_sequence: Complete time series sequence\n",
        "        horizon: Number of timesteps to reserve for testing\n",
        "\n",
        "    Returns:\n",
        "        tuple: (train_seq, test_seq, split_idx) where split_idx is the index where split occurs\n",
        "    \"\"\"\n",
        "\n",
        "    split_idx = len(raw_sequence) - horizon\n",
        "    train_seq = raw_sequence[:split_idx]\n",
        "    test_seq = raw_sequence[split_idx:]\n",
        "    return train_seq, test_seq, split_idx\n",
        "\n",
        "\n",
        "def graph_comparison(title, dataset, mae, original, predictions, split_idx):\n",
        "    \"\"\"\n",
        "    Create visualization comparing original vs predicted values with two plots: full view and zoomed view.\n",
        "\n",
        "    Args:\n",
        "        title: Title for the plots\n",
        "        dataset: DataFrame containing the original data with date index\n",
        "        mae: Mean Absolute Error value to display on plots\n",
        "        original: Array of original/true values\n",
        "        predictions: Array of predicted values\n",
        "        split_idx: Index where training/test split occurs\n",
        "\n",
        "    Returns:\n",
        "        None: Displays two matplotlib plots\n",
        "    \"\"\"\n",
        "\n",
        "    date_index = dataset.index\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.plot(date_index, original, label='Original Price')\n",
        "\n",
        "    test_dates = date_index[split_idx:]\n",
        "    plt.plot(test_dates, predictions, label='Predicted Price')\n",
        "\n",
        "    plt.axvline(x=date_index[split_idx], color='r', linestyle='--', label='Train/Test Split')\n",
        "\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Price')\n",
        "    plt.title(title)\n",
        "\n",
        "    start_date = date_index[len(date_index) - 3000]\n",
        "    end_date = date_index[-1]\n",
        "    plt.xlim(start_date, end_date)\n",
        "\n",
        "    # Format x-axis to show only years\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "    plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
        "\n",
        "    plt.text(0.02, 0.95, f'MAE: {mae:.2f}', transform=plt.gca().transAxes,\n",
        "             bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='lightgray', alpha=0.9),\n",
        "             verticalalignment='top', fontsize=10, color='black')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # === ZOOMED IN PLOT ===\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    plt.plot(date_index, original, label='Original Price')\n",
        "    plt.plot(test_dates, predictions, label='Predicted Price')\n",
        "\n",
        "    plt.axvline(x=date_index[split_idx], color='r', linestyle='--', label='Train/Test Split')\n",
        "\n",
        "    plt.xlabel('Year')\n",
        "    plt.ylabel('Price')\n",
        "    plt.title(title)\n",
        "\n",
        "    start_date = date_index[len(date_index) - 1000]\n",
        "    end_date = date_index[-1]\n",
        "    plt.xlim(start_date, end_date)\n",
        "\n",
        "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y'))\n",
        "    plt.gca().xaxis.set_major_locator(mdates.YearLocator())\n",
        "\n",
        "    plt.text(0.02, 0.95, f'MAE: {mae:.2f}', transform=plt.gca().transAxes,\n",
        "             bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='lightgray', alpha=0.9),\n",
        "             verticalalignment='top', fontsize=10, color='black')\n",
        "\n",
        "    plt.legend()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timesfm[torch]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8WU21-23Ag8l",
        "outputId": "871a733a-35b2-4f8b-f86c-1809bcf7e473"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timesfm[torch]\n",
            "  Downloading timesfm-1.3.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: absl-py>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from timesfm[torch]) (1.4.0)\n",
            "Collecting einshape>=1.0.0 (from timesfm[torch])\n",
            "  Downloading einshape-1.0-py3-none-any.whl.metadata (706 bytes)\n",
            "Requirement already satisfied: huggingface_hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[cli]>=0.23.0->timesfm[torch]) (0.33.4)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from timesfm[torch]) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from timesfm[torch]) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn>=1.2.2 in /usr/local/lib/python3.11/dist-packages (from timesfm[torch]) (1.6.1)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch[cuda]>=2.0.0; python_version == \"3.11\" and extra == \"torch\"->timesfm[torch]) (2.6.0+cu124)\n",
            "Requirement already satisfied: typer>=0.12.3 in /usr/local/lib/python3.11/dist-packages (from timesfm[torch]) (0.16.0)\n",
            "Collecting utilsforecast>=0.1.10 (from timesfm[torch])\n",
            "  Downloading utilsforecast-0.2.12-py3-none-any.whl.metadata (7.6 kB)\n",
            "Requirement already satisfied: wandb>=0.17.5 in /usr/local/lib/python3.11/dist-packages (from timesfm[torch]) (0.21.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm[torch]) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm[torch]) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm[torch]) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm[torch]) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm[torch]) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm[torch]) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm[torch]) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm[torch]) (1.1.5)\n",
            "Collecting InquirerPy==0.3.4 (from huggingface_hub[cli]>=0.23.0->timesfm[torch])\n",
            "  Downloading InquirerPy-0.3.4-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting pfzy<0.4.0,>=0.3.1 (from InquirerPy==0.3.4->huggingface_hub[cli]>=0.23.0->timesfm[torch])\n",
            "  Downloading pfzy-0.3.4-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from InquirerPy==0.3.4->huggingface_hub[cli]>=0.23.0->timesfm[torch]) (3.0.51)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->timesfm[torch]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->timesfm[torch]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->timesfm[torch]) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.2->timesfm[torch]) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.2->timesfm[torch]) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.2.2->timesfm[torch]) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torch[cuda]>=2.0.0; python_version == \"3.11\" and extra == \"torch\"->timesfm[torch]) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torch[cuda]>=2.0.0; python_version == \"3.11\" and extra == \"torch\"->timesfm[torch]) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->torch[cuda]>=2.0.0; python_version == \"3.11\" and extra == \"torch\"->timesfm[torch])\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->torch[cuda]>=2.0.0; python_version == \"3.11\" and extra == \"torch\"->timesfm[torch])\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->torch[cuda]>=2.0.0; python_version == \"3.11\" and extra == \"torch\"->timesfm[torch])\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torch[cuda]>=2.0.0; python_version == \"3.11\" and extra == \"torch\"->timesfm[torch])\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torch[cuda]>=2.0.0; python_version == \"3.11\" and extra == \"torch\"->timesfm[torch])\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torch[cuda]>=2.0.0; python_version == \"3.11\" and extra == \"torch\"->timesfm[torch])\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torch[cuda]>=2.0.0; python_version == \"3.11\" and extra == \"torch\"->timesfm[torch])\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torch[cuda]>=2.0.0; python_version == \"3.11\" and extra == \"torch\"->timesfm[torch])\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torch[cuda]>=2.0.0; python_version == \"3.11\" and extra == \"torch\"->timesfm[torch])\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torch[cuda]>=2.0.0; python_version == \"3.11\" and extra == \"torch\"->timesfm[torch]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torch[cuda]>=2.0.0; python_version == \"3.11\" and extra == \"torch\"->timesfm[torch]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torch[cuda]>=2.0.0; python_version == \"3.11\" and extra == \"torch\"->timesfm[torch]) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torch[cuda]>=2.0.0; python_version == \"3.11\" and extra == \"torch\"->timesfm[torch])\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torch[cuda]>=2.0.0; python_version == \"3.11\" and extra == \"torch\"->timesfm[torch]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torch[cuda]>=2.0.0; python_version == \"3.11\" and extra == \"torch\"->timesfm[torch]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torch[cuda]>=2.0.0; python_version == \"3.11\" and extra == \"torch\"->timesfm[torch]) (1.3.0)\n",
            "\u001b[33mWARNING: torch 2.6.0+cu124 does not provide the extra 'cuda'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->timesfm[torch]) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->timesfm[torch]) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.3->timesfm[torch]) (13.9.4)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.5->timesfm[torch]) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.5->timesfm[torch]) (4.3.8)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.5->timesfm[torch]) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.5->timesfm[torch]) (2.11.7)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb>=0.17.5->timesfm[torch]) (2.33.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.17.5->timesfm[torch]) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.17.5->timesfm[torch]) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.17.5->timesfm[torch]) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb>=0.17.5->timesfm[torch]) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->timesfm[torch]) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm[torch]) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm[torch]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm[torch]) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.23.0->huggingface_hub[cli]>=0.23.0->timesfm[torch]) (2025.7.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer>=0.12.3->timesfm[torch]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer>=0.12.3->timesfm[torch]) (2.19.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torch[cuda]>=2.0.0; python_version == \"3.11\" and extra == \"torch\"->timesfm[torch]) (3.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.17.5->timesfm[torch]) (5.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->timesfm[torch]) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]>=0.23.0->timesfm[torch]) (0.2.13)\n",
            "Downloading einshape-1.0-py3-none-any.whl (21 kB)\n",
            "Downloading InquirerPy-0.3.4-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m128.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading utilsforecast-0.2.12-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timesfm-1.3.0-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.0/55.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pfzy-0.3.4-py3-none-any.whl (8.5 kB)\n",
            "Installing collected packages: pfzy, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, einshape, nvidia-cusparse-cu12, nvidia-cudnn-cu12, InquirerPy, utilsforecast, nvidia-cusolver-cu12, timesfm\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed InquirerPy-0.3.4 einshape-1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pfzy-0.3.4 timesfm-1.3.0 utilsforecast-0.2.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "TimesFM Finetuner: A flexible framework for finetuning TimesFM models on custom datasets.\n",
        "\"\"\"\n",
        "\n",
        "import logging\n",
        "import os\n",
        "from abc import ABC, abstractmethod\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Any, Callable, Dict, List, Optional\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from timesfm.pytorch_patched_decoder import create_quantiles\n",
        "\n",
        "import wandb\n",
        "\n",
        "\n",
        "class MetricsLogger(ABC):\n",
        "  \"\"\"Abstract base class for logging metrics during training.\n",
        "\n",
        "    This class defines the interface for logging metrics during model training.\n",
        "    Concrete implementations can log to different backends (e.g., WandB, TensorBoard).\n",
        "    \"\"\"\n",
        "\n",
        "  @abstractmethod\n",
        "  def log_metrics(self,\n",
        "                  metrics: Dict[str, Any],\n",
        "                  step: Optional[int] = None) -> None:\n",
        "    \"\"\"Log metrics to the specified backend.\n",
        "\n",
        "        Args:\n",
        "          metrics: Dictionary containing metric names and values.\n",
        "          step: Optional step number or epoch for the metrics.\n",
        "        \"\"\"\n",
        "    pass\n",
        "\n",
        "  @abstractmethod\n",
        "  def close(self) -> None:\n",
        "    \"\"\"Clean up any resources used by the logger.\"\"\"\n",
        "    pass\n",
        "\n",
        "\n",
        "class WandBLogger(MetricsLogger):\n",
        "  \"\"\"Weights & Biases implementation of metrics logging.\n",
        "\n",
        "    Args:\n",
        "      project: Name of the W&B project.\n",
        "      config: Configuration dictionary to log.\n",
        "      rank: Process rank in distributed training.\n",
        "    \"\"\"\n",
        "\n",
        "  def __init__(self, project: str, config: Dict[str, Any], rank: int = 0):\n",
        "    self.rank = rank\n",
        "    if rank == 0:\n",
        "      wandb.init(project=project, config=config)\n",
        "\n",
        "  def log_metrics(self,\n",
        "                  metrics: Dict[str, Any],\n",
        "                  step: Optional[int] = None) -> None:\n",
        "    \"\"\"Log metrics to W&B if on the main process.\n",
        "\n",
        "        Args:\n",
        "          metrics: Dictionary of metrics to log.\n",
        "          step: Current training step or epoch.\n",
        "        \"\"\"\n",
        "    if self.rank == 0:\n",
        "      wandb.log(metrics, step=step)\n",
        "\n",
        "  def close(self) -> None:\n",
        "    \"\"\"Finish the W&B run if on the main process.\"\"\"\n",
        "    if self.rank == 0:\n",
        "      wandb.finish()\n",
        "\n",
        "\n",
        "class DistributedManager:\n",
        "  \"\"\"Manages distributed training setup and cleanup.\n",
        "\n",
        "    Args:\n",
        "      world_size: Total number of processes.\n",
        "      rank: Process rank.\n",
        "      master_addr: Address of the master process.\n",
        "      master_port: Port for distributed communication.\n",
        "      backend: PyTorch distributed backend to use.\n",
        "    \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      world_size: int,\n",
        "      rank: int,\n",
        "      master_addr: str = \"localhost\",\n",
        "      master_port: str = \"12358\",\n",
        "      backend: str = \"nccl\",\n",
        "  ):\n",
        "    self.world_size = world_size\n",
        "    self.rank = rank\n",
        "    self.master_addr = master_addr\n",
        "    self.master_port = master_port\n",
        "    self.backend = backend\n",
        "\n",
        "  def setup(self) -> None:\n",
        "    \"\"\"Initialize the distributed environment.\"\"\"\n",
        "    os.environ[\"MASTER_ADDR\"] = self.master_addr\n",
        "    os.environ[\"MASTER_PORT\"] = self.master_port\n",
        "\n",
        "    if not dist.is_initialized():\n",
        "      dist.init_process_group(backend=self.backend,\n",
        "                              world_size=self.world_size,\n",
        "                              rank=self.rank)\n",
        "\n",
        "  def cleanup(self) -> None:\n",
        "    \"\"\"Clean up the distributed environment.\"\"\"\n",
        "    if dist.is_initialized():\n",
        "      dist.destroy_process_group()\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class FinetuningConfig:\n",
        "  \"\"\"Configuration for model training.\n",
        "\n",
        "    Args:\n",
        "      batch_size: Number of samples per batch.\n",
        "      num_epochs: Number of training epochs.\n",
        "      learning_rate: Initial learning rate.\n",
        "      weight_decay: L2 regularization factor.\n",
        "      freq_type: Frequency, can be [0, 1, 2].\n",
        "      use_quantile_loss: bool = False  # Flag to enable/disable quantile loss\n",
        "      quantiles: Optional[List[float]] = None\n",
        "      device: Device to train on ('cuda' or 'cpu').\n",
        "      distributed: Whether to use distributed training.\n",
        "      gpu_ids: List of GPU IDs to use.\n",
        "      master_port: Port for distributed training.\n",
        "      master_addr: Address for distributed training.\n",
        "      use_wandb: Whether to use Weights & Biases logging.\n",
        "      wandb_project: W&B project name.\n",
        "      log_every_n_steps: Log metrics every N steps (batches), this is inspired from Pytorch Lightning\n",
        "      val_check_interval: How often within one training epoch to check val metrics. (also from Pytorch Lightning)\n",
        "        Can be: float (0.0-1.0): fraction of epoch (e.g., 0.5 = validate twice per epoch)\n",
        "                int: validate every N batches\n",
        "    \"\"\"\n",
        "\n",
        "  batch_size: int = 32\n",
        "  num_epochs: int = 20\n",
        "  learning_rate: float = 1e-4\n",
        "  weight_decay: float = 0.01\n",
        "  freq_type: int = 0\n",
        "  use_quantile_loss: bool = False\n",
        "  quantiles: Optional[List[float]] = None\n",
        "  device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  distributed: bool = False\n",
        "  gpu_ids: List[int] = field(default_factory=lambda: [0])\n",
        "  master_port: str = \"12358\"\n",
        "  master_addr: str = \"localhost\"\n",
        "  use_wandb: bool = False\n",
        "  wandb_project: str = \"timesfm-finetuning\"\n",
        "  log_every_n_steps: int = 50\n",
        "  val_check_interval: float = 0.5\n",
        "\n",
        "\n",
        "class TimesFMFinetuner:\n",
        "  \"\"\"Handles model training and validation.\n",
        "\n",
        "    Args:\n",
        "      model: PyTorch model to train.\n",
        "      config: Training configuration.\n",
        "      rank: Process rank for distributed training.\n",
        "      loss_fn: Loss function (defaults to MSE).\n",
        "      logger: Optional logging.Logger instance.\n",
        "    \"\"\"\n",
        "\n",
        "  def __init__(\n",
        "      self,\n",
        "      model: nn.Module,\n",
        "      config: FinetuningConfig,\n",
        "      rank: int = 0,\n",
        "      loss_fn: Optional[Callable] = None,\n",
        "      logger: Optional[logging.Logger] = None,\n",
        "  ):\n",
        "    self.model = model\n",
        "    self.config = config\n",
        "    self.rank = rank\n",
        "    self.logger = logger or logging.getLogger(__name__)\n",
        "    self.device = torch.device(\n",
        "        f\"cuda:{rank}\" if torch.cuda.is_available() else \"cpu\")\n",
        "    self.loss_fn = loss_fn or (lambda x, y: torch.mean((x - y.squeeze(-1))**2))\n",
        "\n",
        "    if config.use_wandb:\n",
        "      self.metrics_logger = WandBLogger(config.wandb_project, config.__dict__,\n",
        "                                        rank)\n",
        "\n",
        "    if config.distributed:\n",
        "      self.dist_manager = DistributedManager(\n",
        "          world_size=len(config.gpu_ids),\n",
        "          rank=rank,\n",
        "          master_addr=config.master_addr,\n",
        "          master_port=config.master_port,\n",
        "      )\n",
        "      self.dist_manager.setup()\n",
        "      self.model = self._setup_distributed_model()\n",
        "\n",
        "  def _setup_distributed_model(self) -> nn.Module:\n",
        "    \"\"\"Configure model for distributed training.\"\"\"\n",
        "    self.model = self.model.to(self.device)\n",
        "    return DDP(self.model,\n",
        "               device_ids=[self.config.gpu_ids[self.rank]],\n",
        "               output_device=self.config.gpu_ids[self.rank])\n",
        "\n",
        "  def _create_dataloader(self, dataset: Dataset, is_train: bool) -> DataLoader:\n",
        "    \"\"\"Create appropriate DataLoader based on training configuration.\n",
        "\n",
        "        Args:\n",
        "          dataset: Dataset to create loader for.\n",
        "          is_train: Whether this is for training (affects shuffling).\n",
        "\n",
        "        Returns:\n",
        "          DataLoader instance.\n",
        "        \"\"\"\n",
        "    if self.config.distributed:\n",
        "      sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "          dataset,\n",
        "          num_replicas=len(self.config.gpu_ids),\n",
        "          rank=dist.get_rank(),\n",
        "          shuffle=is_train)\n",
        "    else:\n",
        "      sampler = None\n",
        "\n",
        "    return DataLoader(\n",
        "        dataset,\n",
        "        batch_size=self.config.batch_size,\n",
        "        shuffle=(is_train and not self.config.distributed),\n",
        "        sampler=sampler,\n",
        "    )\n",
        "\n",
        "  def _quantile_loss(self, pred: torch.Tensor, actual: torch.Tensor,\n",
        "                     quantile: float) -> torch.Tensor:\n",
        "    \"\"\"Calculates quantile loss.\n",
        "        Args:\n",
        "            pred: Predicted values\n",
        "            actual: Actual values\n",
        "            quantile: Quantile at which loss is computed\n",
        "        Returns:\n",
        "            Quantile loss\n",
        "        \"\"\"\n",
        "    dev = actual - pred\n",
        "    loss_first = dev * quantile\n",
        "    loss_second = -dev * (1.0 - quantile)\n",
        "    return 2 * torch.where(loss_first >= 0, loss_first, loss_second)\n",
        "\n",
        "  def _process_batch(self, batch: List[torch.Tensor]) -> tuple:\n",
        "    \"\"\"Process a single batch of data.\n",
        "\n",
        "        Args:\n",
        "          batch: List of input tensors.\n",
        "\n",
        "        Returns:\n",
        "          Tuple of (loss, predictions).\n",
        "        \"\"\"\n",
        "    x_context, x_padding, freq, x_future = [\n",
        "        t.to(self.device, non_blocking=True) for t in batch\n",
        "    ]\n",
        "\n",
        "    predictions = self.model(x_context, x_padding.float(), freq)\n",
        "    predictions_mean = predictions[..., 0]\n",
        "    last_patch_pred = predictions_mean[:, -1, :]\n",
        "\n",
        "    loss = self.loss_fn(last_patch_pred, x_future.squeeze(-1))\n",
        "    if self.config.use_quantile_loss:\n",
        "      quantiles = self.config.quantiles or create_quantiles()\n",
        "      for i, quantile in enumerate(quantiles):\n",
        "        last_patch_quantile = predictions[:, -1, :, i + 1]\n",
        "        loss += torch.mean(\n",
        "            self._quantile_loss(last_patch_quantile, x_future.squeeze(-1),\n",
        "                                quantile))\n",
        "\n",
        "    return loss, predictions\n",
        "\n",
        "  def _train_epoch(self, train_loader: DataLoader,\n",
        "                   optimizer: torch.optim.Optimizer) -> float:\n",
        "    \"\"\"Train for one epoch in a distributed setting.\n",
        "\n",
        "        Args:\n",
        "            train_loader: DataLoader for training data.\n",
        "            optimizer: Optimizer instance.\n",
        "\n",
        "        Returns:\n",
        "            Average training loss for the epoch.\n",
        "        \"\"\"\n",
        "    self.model.train()\n",
        "    total_loss = 0.0\n",
        "    num_batches = len(train_loader)\n",
        "\n",
        "    for batch in train_loader:\n",
        "      loss, _ = self._process_batch(batch)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "\n",
        "    if self.config.distributed:\n",
        "      avg_loss_tensor = torch.tensor(avg_loss, device=self.device)\n",
        "      dist.all_reduce(avg_loss_tensor, op=dist.ReduceOp.SUM)\n",
        "      avg_loss = (avg_loss_tensor / dist.get_world_size()).item()\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "  def _validate(self, val_loader: DataLoader) -> float:\n",
        "    \"\"\"Perform validation.\n",
        "\n",
        "        Args:\n",
        "            val_loader: DataLoader for validation data.\n",
        "\n",
        "        Returns:\n",
        "            Average validation loss.\n",
        "        \"\"\"\n",
        "    self.model.eval()\n",
        "    total_loss = 0.0\n",
        "    num_batches = len(val_loader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for batch in val_loader:\n",
        "        loss, _ = self._process_batch(batch)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "\n",
        "    if self.config.distributed:\n",
        "      avg_loss_tensor = torch.tensor(avg_loss, device=self.device)\n",
        "      dist.all_reduce(avg_loss_tensor, op=dist.ReduceOp.SUM)\n",
        "      avg_loss = (avg_loss_tensor / dist.get_world_size()).item()\n",
        "\n",
        "    return avg_loss\n",
        "\n",
        "  def finetune(self, train_dataset: Dataset,\n",
        "               val_dataset: Dataset) -> Dict[str, Any]:\n",
        "    \"\"\"Train the model.\n",
        "\n",
        "        Args:\n",
        "          train_dataset: Training dataset.\n",
        "          val_dataset: Validation dataset.\n",
        "\n",
        "        Returns:\n",
        "          Dictionary containing training history.\n",
        "        \"\"\"\n",
        "    self.model = self.model.to(self.device)\n",
        "    train_loader = self._create_dataloader(train_dataset, is_train=True)\n",
        "    val_loader = self._create_dataloader(val_dataset, is_train=False)\n",
        "\n",
        "    optimizer = torch.optim.Adam(self.model.parameters(),\n",
        "                                 lr=self.config.learning_rate,\n",
        "                                 weight_decay=self.config.weight_decay)\n",
        "\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"learning_rate\": []}\n",
        "\n",
        "    self.logger.info(\n",
        "        f\"Starting training for {self.config.num_epochs} epochs...\")\n",
        "    self.logger.info(f\"Training samples: {len(train_dataset)}\")\n",
        "    self.logger.info(f\"Validation samples: {len(val_dataset)}\")\n",
        "\n",
        "    try:\n",
        "      for epoch in range(self.config.num_epochs):\n",
        "        train_loss = self._train_epoch(train_loader, optimizer)\n",
        "        val_loss = 0\n",
        "        current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "        metrics = {\n",
        "            \"train_loss\": train_loss,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"learning_rate\": current_lr,\n",
        "            \"epoch\": epoch + 1,\n",
        "        }\n",
        "\n",
        "        if self.config.use_wandb:\n",
        "          self.metrics_logger.log_metrics(metrics)\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"learning_rate\"].append(current_lr)\n",
        "\n",
        "        if self.rank == 0:\n",
        "          self.logger.info(\n",
        "              f\"[Epoch {epoch+1}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\"\n",
        "          )\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "      self.logger.info(\"Training interrupted by user\")\n",
        "\n",
        "    if self.config.distributed:\n",
        "      self.dist_manager.cleanup()\n",
        "\n",
        "    if self.config.use_wandb:\n",
        "      self.metrics_logger.close()\n",
        "\n",
        "    return {\"history\": history}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovIb1y7wr3yz",
        "outputId": "fc449541-aa67-438c-b9f5-5e055dbb55f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " See https://github.com/google-research/timesfm/blob/master/README.md for updated APIs.\n",
            "Loaded PyTorch TimesFM, likely because python version is 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.multiprocessing as mp\n",
        "import yfinance as yf\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "from timesfm import TimesFm, TimesFmCheckpoint, TimesFmHparams\n",
        "from timesfm.pytorch_patched_decoder import PatchedTimeSeriesDecoder\n",
        "import os\n",
        "\n",
        "\n",
        "class TimeSeriesDataset(Dataset):\n",
        "  \"\"\"Dataset for time series data compatible with TimesFM.\"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               series: np.ndarray,\n",
        "               context_length: int,\n",
        "               horizon_length: int,\n",
        "               freq_type: int = 0):\n",
        "    \"\"\"\n",
        "        Initialize dataset.\n",
        "\n",
        "        Args:\n",
        "            series: Time series data\n",
        "            context_length: Number of past timesteps to use as input\n",
        "            horizon_length: Number of future timesteps to predict\n",
        "            freq_type: Frequency type (0, 1, or 2)\n",
        "        \"\"\"\n",
        "    if freq_type not in [0, 1, 2]:\n",
        "      raise ValueError(\"freq_type must be 0, 1, or 2\")\n",
        "\n",
        "    self.series = series\n",
        "    self.context_length = context_length\n",
        "    self.horizon_length = horizon_length\n",
        "    self.freq_type = freq_type\n",
        "    self._prepare_samples()\n",
        "\n",
        "  def _prepare_samples(self) -> None:\n",
        "    \"\"\"Prepare sliding window samples from the time series.\"\"\"\n",
        "    self.samples = []\n",
        "    total_length = self.context_length + self.horizon_length\n",
        "\n",
        "    for start_idx in range(0, len(self.series) - total_length + 1):\n",
        "      end_idx = start_idx + self.context_length\n",
        "      x_context = self.series[start_idx:end_idx]\n",
        "      x_future = self.series[end_idx:end_idx + self.horizon_length]\n",
        "      self.samples.append((x_context, x_future))\n",
        "\n",
        "  def __len__(self) -> int:\n",
        "    return len(self.samples)\n",
        "\n",
        "  def __getitem__(\n",
        "      self, index: int\n",
        "  ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "    x_context, x_future = self.samples[index]\n",
        "\n",
        "    x_context = torch.tensor(x_context, dtype=torch.float32)\n",
        "    x_future = torch.tensor(x_future, dtype=torch.float32)\n",
        "\n",
        "    input_padding = torch.zeros_like(x_context)\n",
        "    freq = torch.tensor([self.freq_type], dtype=torch.long)\n",
        "\n",
        "    return x_context, input_padding, freq, x_future\n",
        "\n",
        "def prepare_datasets(series: np.ndarray,\n",
        "                     context_length: int,\n",
        "                     horizon_length: int,\n",
        "                     freq_type: int = 0,\n",
        "                     train_split: float = 0.8) -> Tuple[Dataset, Dataset]:\n",
        "  \"\"\"\n",
        "    Prepare training and validation datasets from time series data.\n",
        "\n",
        "    Args:\n",
        "        series: Input time series data\n",
        "        context_length: Number of past timesteps to use\n",
        "        horizon_length: Number of future timesteps to predict\n",
        "        freq_type: Frequency type (0, 1, or 2)\n",
        "        train_split: Fraction of data to use for training\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (train_dataset, val_dataset)\n",
        "    \"\"\"\n",
        "  train_size = int(len(series) * train_split)\n",
        "  train_data = series[:train_size]\n",
        "  val_data = series[train_size:]\n",
        "\n",
        "  # Create datasets with specified frequency type\n",
        "  train_dataset = TimeSeriesDataset(train_data,\n",
        "                                    context_length=context_length,\n",
        "                                    horizon_length=horizon_length,\n",
        "                                    freq_type=freq_type)\n",
        "\n",
        "  val_dataset = TimeSeriesDataset(val_data,\n",
        "                                  context_length=context_length,\n",
        "                                  horizon_length=horizon_length,\n",
        "                                  freq_type=freq_type)\n",
        "\n",
        "  return train_dataset, val_dataset"
      ],
      "metadata": {
        "id": "SzIeU-ayp386"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(load_weights: bool = False):\n",
        "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "  repo_id = \"google/timesfm-2.0-500m-pytorch\"\n",
        "  hparams = TimesFmHparams(\n",
        "      backend=device,\n",
        "      per_core_batch_size=32,\n",
        "      horizon_len=128,\n",
        "      num_layers=50,\n",
        "      use_positional_embedding=False,\n",
        "      context_len=\n",
        "      2048,  # Context length can be anything up to 2048 in multiples of 32\n",
        "  )\n",
        "  tfm = TimesFm(hparams=hparams,\n",
        "                checkpoint=TimesFmCheckpoint(huggingface_repo_id=repo_id))\n",
        "  print('TFM HRZ LEN', tfm.horizon_len)\n",
        "  model = PatchedTimeSeriesDecoder(tfm._model_config)\n",
        "\n",
        "  print('MODEL HRZ LEN', tfm._model_config.horizon_len)\n",
        "  if load_weights:\n",
        "    checkpoint_path = path.join(snapshot_download(repo_id), \"torch_model.ckpt\")\n",
        "    loaded_checkpoint = torch.load(checkpoint_path, weights_only=True)\n",
        "    model.load_state_dict(loaded_checkpoint)\n",
        "  return model, hparams, tfm._model_config"
      ],
      "metadata": {
        "id": "KMk_eq72p6_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(\n",
        "    model: TimesFm,\n",
        "    val_dataset: Dataset,\n",
        "    save_path: Optional[str] = \"predictions.png\",\n",
        ") -> None:\n",
        "  \"\"\"\n",
        "    Plot model predictions against ground truth for a batch of validation data.\n",
        "\n",
        "    Args:\n",
        "      model: Trained TimesFM model\n",
        "      val_dataset: Validation dataset\n",
        "      save_path: Path to save the plot\n",
        "    \"\"\"\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  model.eval()\n",
        "\n",
        "  x_context, x_padding, freq, x_future = val_dataset[0]\n",
        "  x_context = x_context.unsqueeze(0)  # Add batch dimension\n",
        "  x_padding = x_padding.unsqueeze(0)\n",
        "  freq = freq.unsqueeze(0)\n",
        "  x_future = x_future.unsqueeze(0)\n",
        "\n",
        "  device = next(model.parameters()).device\n",
        "  x_context = x_context.to(device)\n",
        "  x_padding = x_padding.to(device)\n",
        "  freq = freq.to(device)\n",
        "  x_future = x_future.to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    predictions = model(x_context, x_padding.float(), freq)\n",
        "    predictions_mean = predictions[..., 0]  # [B, N, horizon_len]\n",
        "    last_patch_pred = predictions_mean[:, -1, :]  # [B, horizon_len]\n",
        "\n",
        "  context_vals = x_context[0].cpu().numpy()\n",
        "  future_vals = x_future[0].cpu().numpy()\n",
        "  pred_vals = last_patch_pred[0].cpu().numpy()\n",
        "\n",
        "  context_len = len(context_vals)\n",
        "  horizon_len = len(future_vals)\n",
        "\n",
        "  plt.figure(figsize=(12, 6))\n",
        "\n",
        "  plt.plot(range(context_len),\n",
        "           context_vals,\n",
        "           label=\"Historical Data\",\n",
        "           color=\"blue\",\n",
        "           linewidth=2)\n",
        "\n",
        "  plt.plot(\n",
        "      range(context_len, context_len + horizon_len),\n",
        "      future_vals,\n",
        "      label=\"Ground Truth\",\n",
        "      color=\"green\",\n",
        "      linestyle=\"--\",\n",
        "      linewidth=2,\n",
        "  )\n",
        "\n",
        "  print(pred_vals)\n",
        "  print(len(pred_vals))\n",
        "\n",
        "  plt.plot(range(context_len, context_len + horizon_len),\n",
        "           pred_vals,\n",
        "           label=\"Prediction\",\n",
        "           color=\"red\",\n",
        "           linewidth=2)\n",
        "\n",
        "  plt.xlabel(\"Time Step\")\n",
        "  plt.ylabel(\"Value\")\n",
        "  plt.title(\"TimesFM Predictions vs Ground Truth\")\n",
        "  plt.legend()\n",
        "  plt.grid(True)\n",
        "\n",
        "  if save_path:\n",
        "    plt.savefig(save_path)\n",
        "    print(f\"Plot saved to {save_path}\")\n",
        "\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "B4AHKbDyp8ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(context_len: int,\n",
        "             horizon_len: int,\n",
        "             freq_type: int = 0) -> Tuple[Dataset, Dataset]:\n",
        "  df = getDataFrame()\n",
        "  time_series = getPrice(df).values[:-730].reshape(-1, 1)\n",
        "\n",
        "  train_dataset, val_dataset = prepare_datasets(\n",
        "      series=time_series,\n",
        "      context_length=context_len,\n",
        "      horizon_length=horizon_len,\n",
        "      freq_type=freq_type,\n",
        "      train_split=0.9,\n",
        "  )\n",
        "\n",
        "  print(f\"Created datasets:\")\n",
        "  print(f\"- Training samples: {len(train_dataset)}\")\n",
        "  print(f\"- Validation samples: {len(val_dataset)}\")\n",
        "  print(f\"- Using frequency type: {freq_type}\")\n",
        "  return train_dataset, val_dataset\n",
        "\n",
        "\n",
        "\n",
        "def single_gpu_example():\n",
        "  model, hparams, tfm_config = get_model(load_weights=True)\n",
        "  config = FinetuningConfig(batch_size=256,\n",
        "                            num_epochs=15,\n",
        "                            learning_rate=0.0001,\n",
        "                            use_wandb=True,\n",
        "                            freq_type=1,\n",
        "                            log_every_n_steps=1,\n",
        "                            val_check_interval=0.5,\n",
        "                            use_quantile_loss=True)\n",
        "\n",
        "  train_dataset, val_dataset = get_data(128,\n",
        "                                        tfm_config.horizon_len,\n",
        "                                        freq_type=config.freq_type)\n",
        "  finetuner = TimesFMFinetuner(model, config)\n",
        "\n",
        "  print(\"\\nStarting finetuning...\")\n",
        "  results = finetuner.finetune(train_dataset=train_dataset,\n",
        "                               val_dataset=val_dataset)\n",
        "\n",
        "  print(\"\\nFinetuning completed!\")\n",
        "  print(f\"Training history: {len(results['history']['train_loss'])} epochs\")\n",
        "\n",
        "  plot_predictions(\n",
        "      model=model,\n",
        "      val_dataset=val_dataset,\n",
        "      save_path=\"timesfm_predictions.png\",\n",
        "  )\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "yzsJFqPIp9Br"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL = single_gpu_example()"
      ],
      "metadata": {
        "id": "gJE7-TPKr-Ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def predict_autoregressive(model, context_data, freq_type=1, context_length=736,\n",
        "                           prediction_length=730, step_size=64):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "\n",
        "    if len(context_data) < context_length:\n",
        "        raise ValueError(f\"Need at least {context_length} data points for prediction\")\n",
        "\n",
        "    # Initialize with historical data\n",
        "    extended_data = context_data.copy()\n",
        "    all_predictions = []\n",
        "\n",
        "    steps_needed = (prediction_length + step_size - 1) // step_size  # Ceiling division\n",
        "\n",
        "    print(f\"Making {steps_needed} autoregressive steps to predict {prediction_length} points...\")\n",
        "\n",
        "    for step in range(steps_needed):\n",
        "        current_context = extended_data[-context_length:]\n",
        "\n",
        "        x_context = torch.tensor(current_context, dtype=torch.float32).unsqueeze(0)\n",
        "        x_padding = torch.zeros_like(x_context)\n",
        "        freq = torch.tensor([freq_type], dtype=torch.long).unsqueeze(0)\n",
        "\n",
        "        x_context = x_context.to(device)\n",
        "        x_padding = x_padding.to(device)\n",
        "        freq = freq.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = model(x_context, x_padding.float(), freq)\n",
        "            predictions_mean = predictions[..., 0]\n",
        "            forecast = predictions_mean[0, -1, :].cpu().numpy()\n",
        "\n",
        "        remaining_predictions = prediction_length - len(all_predictions)\n",
        "        predictions_to_use = min(step_size, remaining_predictions, len(forecast))\n",
        "\n",
        "        step_predictions = forecast[:predictions_to_use]\n",
        "        all_predictions.extend(step_predictions)\n",
        "\n",
        "        extended_data = np.concatenate([extended_data.flatten(), step_predictions])\n",
        "\n",
        "        print(f\"Step {step + 1}/{steps_needed}: Made {len(step_predictions)} predictions\")\n",
        "\n",
        "        if len(all_predictions) >= prediction_length:\n",
        "            break\n",
        "\n",
        "    return np.array(all_predictions[:prediction_length])\n",
        "\n",
        "\n",
        "def main_custom_length():\n",
        "    finetuned_model = MODEL\n",
        "\n",
        "    df = getDataFrame()\n",
        "    recent_data, test_seq, split_idx = split_train_test(getPrice(df), 730)\n",
        "    print('recent', recent_data)\n",
        "\n",
        "    plt.figure(figsize=(20, 15))\n",
        "\n",
        "\n",
        "    predictions = predict_autoregressive(finetuned_model, recent_data,\n",
        "                                         prediction_length=730, step_size=64)\n",
        "    actual = recent_data[-730:]\n",
        "    mae = calculateMAE(actual, predictions)\n",
        "    graph_comparison('S&P 500 TimesFM-2.0-500M (Fine-Tuned) Prediction', df, mae, df,\n",
        "                     predictions, split_idx)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_custom_length()\n"
      ],
      "metadata": {
        "id": "RgaX_WJ4UCzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub[cli]\n",
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "CgZiLz-GEt49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.save(MODEL.state_dict(), \"pytorch_model.bin\")"
      ],
      "metadata": {
        "id": "aOtrKGiyFR1r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}